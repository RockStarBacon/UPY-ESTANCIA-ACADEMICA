ğŸŒ¦ï¸ Climate Data Pipeline with Apache Airflow

Este proyecto implementa un pipeline de datos automatizado para la extracciÃ³n, transformaciÃ³n y carga (ETL) de informaciÃ³n climÃ¡tica desde Weather Underground, utilizando Apache Airflow, PostgreSQL y Docker.

El sistema permite recolectar datos histÃ³ricos y actuales de mÃºltiples estaciones meteorolÃ³gicas y almacenarlos de forma estructurada para su posterior anÃ¡lisis.

ğŸ“Œ Arquitectura del Proyecto

El pipeline sigue una arquitectura ETL orquestada con Airflow:

Extract

Web scraping de datos climÃ¡ticos desde Weather Underground

MÃºltiples estaciones meteorolÃ³gicas

Manejo de fechas y rangos histÃ³ricos

Transform

Limpieza de datos

ConversiÃ³n de direcciones de viento a Ã¡ngulos

NormalizaciÃ³n de valores

EstructuraciÃ³n en DataFrames con Pandas

Load

InserciÃ³n de datos en PostgreSQL

Persistencia de datos histÃ³ricos y actuales

ğŸ§° TecnologÃ­as Utilizadas

Python 3.9

Apache Airflow 2.9.1

PostgreSQL 15

Docker & Docker Compose

Pandas

BeautifulSoup

Requests

SQLAlchemy

ğŸ“‚ Estructura del Proyecto
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ DAG.py              # DAG de Airflow (ETL automatizado)
â”œâ”€â”€ CARGA.py                # Script de carga histÃ³rica de datos
â”œâ”€â”€ docker-compose.yml      # Infraestructura Docker (Airflow + PostgreSQL)
â”œâ”€â”€ init-db.sh              # Script de inicializaciÃ³n de base de datos
â””â”€â”€ README.md

âš™ï¸ DescripciÃ³n de Archivos
ğŸ” DAG.py

Define un DAG de Airflow

Ejecuta el proceso ETL de forma programada

Extrae datos climÃ¡ticos diarios

Transforma variables como direcciÃ³n del viento

Carga los datos en PostgreSQL

Incluye:

Reintentos automÃ¡ticos

Manejo de errores

Control de fechas

ğŸ“¦ CARGA.py

Script independiente para carga histÃ³rica

Recorre un rango de fechas (2023â€“2025)

Extrae datos por estaciÃ³n y por dÃ­a

Ideal para poblar la base de datos inicial

ğŸ³ docker-compose.yml

Levanta toda la infraestructura necesaria:

PostgreSQL

Airflow Webserver

Airflow Scheduler

Incluye:

Persistencia de datos con volÃºmenes

Zona horaria configurada (America/Merida)

Variables de entorno seguras

Puertos expuestos:

PostgreSQL: 5432

Airflow UI: 8080

ğŸš€ InstalaciÃ³n y EjecuciÃ³n
1ï¸âƒ£ Clonar el repositorio
git clone https://github.com/tu-usuario/tu-repo.git
cd tu-repo

2ï¸âƒ£ Levantar los servicios con Docker
docker-compose up -d

3ï¸âƒ£ Acceder a Airflow

URL: http://localhost:8080

Usuario y contraseÃ±a por defecto (segÃºn configuraciÃ³n de Airflow)

ğŸ§ª EjecuciÃ³n del Pipeline

Activa el DAG desde la interfaz de Airflow

El scheduler ejecutarÃ¡ automÃ¡ticamente el proceso ETL

Los datos se almacenan en PostgreSQL

ğŸ—„ï¸ Base de Datos

Motor: PostgreSQL

Base de datos: prueba_pipeline

Contiene datos climÃ¡ticos por:

EstaciÃ³n

Fecha

Variables meteorolÃ³gicas

DirecciÃ³n del viento convertida a Ã¡ngulo

ğŸ¯ Objetivo del Proyecto

Este proyecto tiene como objetivo:

Aplicar conceptos de Data Engineering

DiseÃ±ar un pipeline reproducible y escalable

Integrar orquestaciÃ³n de datos, scraping, y almacenamiento

Servir como base para anÃ¡lisis climÃ¡tico o modelos predictivos

ğŸ“Œ Posibles Mejoras

Implementar validaciÃ³n de datos (Great Expectations)

Agregar visualizaciÃ³n con dashboards

Migrar a un entorno distribuido

AÃ±adir alertas y monitoreo

ğŸ‘¨â€ğŸ’» Autor

Alejandro Caballero
Proyecto acadÃ©mico / Data Engineering
